HW2
================
Miriam Lachs
2024-09-27

# Problem 1

First let’s import and clean the data

``` r
subway_df =
  read_csv("NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(route8 = as.character(route8),
         route9 = as.character(route9),
         route10 = as.character(route10),
         route11 = as.character(route11)) %>% 
  pivot_longer(cols = route1:route11,values_to = 'route',names_to = 'route_num') %>% 
  select(line,station_name,station_latitude,station_longitude,entry,vending,entrance_type,ada,route_num,route) %>% 
  mutate(entry = ifelse(entry=='YES',TRUE,FALSE))
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

This data set contains information about the NYC Subway. It contains
variables about the various lines and subway stations, including the
name, the stations location in latitude and longitude, the various
routes it service, the types of entrances and weather its ADA compliant.

To clean this data first the column names were cleaned. Next some of the
route variables were converted to character vectors. This was done to
make all the routes the same, so they could be pivoted. That was the
next step. After that the columns of interest were selected. Finally,
the entry variable was converted to a logical vector. The final
dimensions of the table are 20548 rows x 10 columns.This data is
relatively tidy since we pivoted the route variables.

``` r
subway_df %>% 
  distinct(line,station_name) %>% 
  count()
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1   465

There are 465 distinct subway stations.

``` r
subway_df %>% 
  distinct(line,station_name,ada) %>% 
  count(ada)
```

    ## # A tibble: 2 × 2
    ##   ada       n
    ##   <lgl> <int>
    ## 1 FALSE   381
    ## 2 TRUE     84

There are 84 ADA compliant stations.

``` r
subway_df %>% 
  filter(vending=='NO',entry==TRUE) %>% 
  count()
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1   759

``` r
subway_df %>% 
  filter(vending=='NO') %>% 
  count()
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1  2013

``` r
759/2013
```

    ## [1] 0.3770492

37.7% of station entrances / exits without vending allow entrance

``` r
subway_df %>% 
  filter(route=='A') %>% 
  distinct(line,station_name) %>% count()
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1    60

``` r
subway_df %>% 
  filter(route=='A') %>% 
  distinct(line,station_name,ada) %>% count(ada)
```

    ## # A tibble: 2 × 2
    ##   ada       n
    ##   <lgl> <int>
    ## 1 FALSE    43
    ## 2 TRUE     17

There are 60 stations that serve the A train, 17 of which are ADA
compliant.

## Problem 2

Load and clean Mr.Trash Wheel Data

``` r
mr_tw_df=
  read_excel('202309 Trash Wheel Collection Data.xlsx',sheet = 'Mr. Trash Wheel',range = 'A2:N586') %>% 
  janitor::clean_names() %>% 
  mutate(sports_balls = as.integer(sports_balls), trash_wheel ='MR', year = as.numeric(year), date=make_date(year,month(date),day(date)))

professor_tw_df=
  read_excel('202309 Trash Wheel Collection Data.xlsx',sheet = 'Professor Trash Wheel',range = 'A2:M108') %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = 'PROFESSOR',date=make_date(year,month(date),day(date)))

gwynnda_tw_df=
  read_excel('202309 Trash Wheel Collection Data.xlsx',sheet = 'Gwynnda Trash Wheel',range = 'A2:L157') %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = 'GWYNNDA',date=make_date(year,month(date),day(date)))
```

join the data sets

``` r
full_tw_df= bind_rows(mr_tw_df,professor_tw_df,gwynnda_tw_df)
```

The full trash wheel data set is comprised of the data from three
different trash wheels, Mr. Trash Wheel, Professor Trash Wheel and
Gwynnda Trash Wheel. The data looks at trash collected from 2014-05-16
to 2023-06-30 with a total of 845 observations. While there are 15
variables only 13 are shared by all three wheels. The mean amount of
trash collected is 3.0094793 tons. While the total number of homes
powered is NA.

``` r
full_tw_df %>% 
  filter(trash_wheel=='PROFESSOR') %>% 
  select(weight_tons) %>% 
  sum()
```

    ## [1] 216.26

The total weight collected by Professor trash wheel is 216.26 tons.

``` r
full_tw_df %>% 
  filter(trash_wheel=='GWYNNDA',month=='June',year==2022) %>% 
  select(cigarette_butts) %>% 
  sum()
```

    ## [1] 18120

The total number of cigarette butts collected by Gwynnda in June of 2022
was 18120

## Problem 3

Gather and clean the data sets

``` r
bakers_df=
  read_csv("gbb_datasets/bakers.csv") %>% 
  janitor::clean_names() %>% 
  separate(baker_name," ", into=c('first_name','last_name'))
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df =
  read_csv("gbb_datasets/bakes.csv") %>% 
  janitor::clean_names() %>% 
  mutate(baker = gsub('"','',baker))
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results_df =
  read_csv("gbb_datasets/results.csv", skip = 2) %>% 
  janitor::clean_names() %>% 
  mutate(baker=ifelse((series==2&baker=='Joanne'),'Jo',baker))
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Combine data sets

``` r
full_gbb_df=
 bakes_df %>% 
  full_join(results_df, join_by(series==series,episode==episode,baker==baker)) %>% 
  full_join(bakers_df, join_by(series==series,baker==first_name)) %>% 
  mutate(first_name=baker) %>% 
  select(series,episode,signature_bake,show_stopper,technical,result,first_name,last_name,baker_age,baker_occupation,hometown)


write_csv(full_gbb_df, file = 'gbb_datasets/full_gbb.csv')
```

There were multiple different steps to cleaning and joining these data
sets. For the bakers data set in order to be incorporated with the other
data sets the bakers names needed to be split into first and last names.
For the bakers data set, there were extra ‘“’ in the some of the names
that needed to be removed. This name continued to be a problem as it was
under a different value in the results data set. The final data set
contains the information from all three detailing each episode, what was
made, the resulting scores and placements of the bakes, and finally who
baked them as well as details on those individual bakers. I found this
problem to be particularly frustrating, especially coming from a data
engineering background. In terms of data warehousing, data like the
original data sets are separated on purpose to reduce unnecessary
replication of information and to increase clarity and readability of
data. This larger combined data set goes beyond joining useful
information together and becomes so large it becomes cumbersome.

``` r
star_df =
full_gbb_df%>% 
  select(series,episode, result,first_name,last_name) %>% 
  filter((result=='WINNER'|result=='STAR BAKER')&series>4) %>% 
  arrange(series,episode)
```

It seems like winning Star Baker towards the end of season, may be
indicative of winning overall. However it is not a guarantee There are
cirtenly some surprises when just looking at this data, for example
season 5 or season 10.

``` r
viewers_df =
  read_csv("gbb_datasets/viewers.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(cols = series_1:series_10,names_to = 'series', values_to = 'viewers',names_prefix = "series_") %>% 
  mutate(series=as.numeric(series))
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
viewers_df
```

    ## # A tibble: 100 × 3
    ##    episode series viewers
    ##      <dbl>  <dbl>   <dbl>
    ##  1       1      1    2.24
    ##  2       1      2    3.1 
    ##  3       1      3    3.85
    ##  4       1      4    6.6 
    ##  5       1      5    8.51
    ##  6       1      6   11.6 
    ##  7       1      7   13.6 
    ##  8       1      8    9.46
    ##  9       1      9    9.55
    ## 10       1     10    9.62
    ## # ℹ 90 more rows

``` r
viewers_df %>% 
  filter(series==1) %>%
  summarise(mean=mean(viewers, na.rm = TRUE))
```

    ## # A tibble: 1 × 1
    ##    mean
    ##   <dbl>
    ## 1  2.77

``` r
viewers_df %>% 
  filter(series==5) %>%
  summarise(mean=mean(viewers, na.rm = TRUE))
```

    ## # A tibble: 1 × 1
    ##    mean
    ##   <dbl>
    ## 1  10.0
