---
title: "HW2"
author: "Miriam Lachs"
date: "2024-09-27"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
```

# Problem 1 

First let's import and clean the data

```{r}
subway_df =
  read_csv("NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(route8 = as.character(route8),
         route9 = as.character(route9),
         route10 = as.character(route10),
         route11 = as.character(route11)) %>% 
  pivot_longer(cols = route1:route11,values_to = 'route',names_to = 'route_num') %>% 
  select(line,station_name,station_latitude,station_longitude,entry,vending,entrance_type,ada,route_num,route) %>% 
  mutate(entry = ifelse(entry=='YES',TRUE,FALSE))
```

This data set contains information about the NYC Subway. It contains variables about the various lines and subway stations, including the name, the stations location in latitude and longitude, the various routes it service, the types of entrances and weather its ADA compliant. 

To clean this data first the column names were cleaned. Next some of the route variables were converted to character vectors. This was done to make all the routes the same, so they could be pivoted. That was the next step. After that the columns of interest were selected. Finally, the entry variable was converted to a logical vector. The final dimensions of the table are 20548 rows x 10 columns.This data is relatively tidy since we pivoted the route variables. 

```{r}
subway_df %>% 
  distinct(line,station_name) %>% 
  count()
```

There are 465 distinct subway stations.

```{r}
subway_df %>% 
  distinct(line,station_name,ada) %>% 
  count(ada)
```

There are 84 ADA compliant stations.

```{r}
subway_df %>% 
  filter(vending=='NO',entry==TRUE) %>% 
  count()
subway_df %>% 
  filter(vending=='NO') %>% 
  count()

759/2013
```
37.7% of station entrances / exits without vending allow entrance

```{r}
subway_df %>% 
  filter(route=='A') %>% 
  distinct(line,station_name) %>% count()

subway_df %>% 
  filter(route=='A') %>% 
  distinct(line,station_name,ada) %>% count(ada)
```
There are 60 stations that serve the A train, 17 of which are ADA compliant.

## Problem 2

Load and clean Mr.Trash Wheel Data

```{r}
mr_tw_df=
  read_excel('202309 Trash Wheel Collection Data.xlsx',sheet = 'Mr. Trash Wheel',range = 'A2:N586') %>% 
  janitor::clean_names() %>% 
  mutate(sports_balls = as.integer(sports_balls), trash_wheel ='MR', year = as.numeric(year), date=make_date(year,month(date),day(date)))

professor_tw_df=
  read_excel('202309 Trash Wheel Collection Data.xlsx',sheet = 'Professor Trash Wheel',range = 'A2:M108') %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = 'PROFESSOR',date=make_date(year,month(date),day(date)))

gwynnda_tw_df=
  read_excel('202309 Trash Wheel Collection Data.xlsx',sheet = 'Gwynnda Trash Wheel',range = 'A2:L157') %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = 'GWYNNDA',date=make_date(year,month(date),day(date)))
```

join the data sets

```{r}
full_tw_df= bind_rows(mr_tw_df,professor_tw_df,gwynnda_tw_df)

```

The full trash wheel data set is comprised of the data from three different trash wheels, Mr. Trash Wheel, Professor Trash Wheel and Gwynnda Trash Wheel. The data looks at trash collected from `r min(pull(full_tw_df,date))` to `r max(pull(full_tw_df,date))` with a total of `r count(full_tw_df)` observations. While there are `r length(full_tw_df)` variables only 13 are shared by all three wheels. The mean amount of trash collected is `r mean(pull(full_tw_df,weight_tons))` tons. While the total number of homes powered is `r sum(pull(full_tw_df,homes_powered))`. 

```{r}
full_tw_df %>% 
  filter(trash_wheel=='PROFESSOR') %>% 
  select(weight_tons) %>% 
  sum()
```

The total weight collected by Professor trash wheel is 216.26 tons.

```{r}
full_tw_df %>% 
  filter(trash_wheel=='GWYNNDA',month=='June',year==2022) %>% 
  select(cigarette_butts) %>% 
  sum()
```

The total number of cigarette butts collected by Gwynnda in June of 2022 was 18120

## Problem 3

Gather and clean the data sets
```{r}
bakers_df=
  read_csv("gbb_datasets/bakers.csv") %>% 
  janitor::clean_names() %>% 
  separate(baker_name," ", into=c('first_name','last_name'))
bakes_df =
  read_csv("gbb_datasets/bakes.csv") %>% 
  janitor::clean_names() %>% 
  mutate(baker = gsub('"','',baker))

results_df =
  read_csv("gbb_datasets/results.csv", skip = 2) %>% 
  janitor::clean_names() %>% 
  mutate(baker=ifelse((series==2&baker=='Joanne'),'Jo',baker))

```

Combine data sets
```{r}
full_gbb_df=
 bakes_df %>% 
  full_join(results_df, join_by(series==series,episode==episode,baker==baker)) %>% 
  full_join(bakers_df, join_by(series==series,baker==first_name)) %>% 
  mutate(first_name=baker) %>% 
  select(series,episode,signature_bake,show_stopper,technical,result,first_name,last_name,baker_age,baker_occupation,hometown)


write_csv(full_gbb_df, file = 'gbb_datasets/full_gbb.csv')

```

There were multiple different steps to cleaning and joining these data sets. For the bakers data set in order to be incorporated with the other data sets the bakers names needed to be split into first and last names. For the bakers data set, there were extra '"' in the some of the names that needed to be removed. This name continued to be a problem as it was under a different value in the results data set. 
The final data set contains the information from all three detailing each episode, what was made, the resulting scores and placements of the bakes, and finally who baked them as well as details on those individual bakers. I found this problem to be particularly frustrating, especially coming from a data engineering background. In terms of data warehousing, data like the original data sets are separated on purpose to reduce unnecessary replication of information and to increase clarity and readability of data. This larger combined data set goes beyond joining useful information together and becomes so large it becomes cumbersome.

```{r}
star_df =
full_gbb_df%>% 
  select(series,episode, result,first_name,last_name) %>% 
  filter((result=='WINNER'|result=='STAR BAKER')&series>4) %>% 
  arrange(series,episode)
```

It seems like winning  Star Baker towards the end of season, may be indicative of winning overall. However it is not a guarantee There are cirtenly some surprises when just looking at this data, for example season 5 or season 10.

```{r}

viewers_df =
  read_csv("gbb_datasets/viewers.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(cols = series_1:series_10,names_to = 'series', values_to = 'viewers',names_prefix = "series_") %>% 
  mutate(series=as.numeric(series))

viewers_df
```

```{r}
viewers_df %>% 
  filter(series==1) %>%
  summarise(mean=mean(viewers, na.rm = TRUE))
```

```{r}
viewers_df %>% 
  filter(series==5) %>%
  summarise(mean=mean(viewers, na.rm = TRUE))
```
